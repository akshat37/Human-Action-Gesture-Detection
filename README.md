# Human-Action-Gesture-Detection
Real-Time Human Pose Detection using YOLOv8 This project uses the Ultralytics YOLOv8 pose model to perform human pose estimation on a video file. It saves both the full frames and cropped images of detected persons, and logs keypoints data into a CSV file. ğŸ“ Project Structure suspicious.mp4 - Input video file yolo11s-pose.pt - Pretrained YOLOv8 pose model images/ - Folder for full-frame images images1/ - Folder for cropped person images nkeypoint.csv - Output CSV with keypoint data notebook.ipynb - Jupyter notebook (your code) requirements.txt - Python dependencies README.md - Project documentation

ğŸ“¦ Requirements Install required Python libraries using: pip install -r requirements.txt ğŸ§  Model Used YOLOv8 Pose from Ultralytics (https://github.com/ultralytics/ultralytics) The model detects human body keypoints (e.g., shoulders, elbows, knees) in real time. ğŸ› ï¸ What It Does

Loads a video (suspicious.mp4)
Reads every frame from the video (no skipping)
Uses YOLOv8 to detect people and their keypoints
Saves:
Full-size frames â†’ images/
Cropped detections â†’ images1/
Keypoints â†’ nkeypoint.csv
ğŸ“Š Output Example images/img_12.jpg: full frame images1/person_nn_1545.jpg: cropped person nkeypoint.csv: image_name, x0, y0, x1, y1, ..., x16, y16 person_nn_1545.jpg, 0.34, 0.12, ..., 0.89, 0.77

ğŸ§ª How to Run (Notebook)

Launch the Jupyter Notebook: jupyter notebook

Open the provided notebook (e.g., pose_detect.ipynb or similar).

Run the code. It will:

Process the video
Save images
Generate nkeypoint.csv

ğŸ“ Notes
You can change the confidence threshold (default: 0.5) to control the detection sensitivity.
For larger or more accurate models, try yolov8m-pose.pt or yolov8l-pose.pt.

ğŸ¤– Future Enhancements
Real-time webcam detection
Action recognition based on keypoints
Visualize skeletons on saved frames

ğŸ‘¨â€ğŸ’» Author
swayam sidgor
akshat pandey
risabh tadwal
krishnakant mehra
Project from MANIT Bhopal Internship (BFBF)
